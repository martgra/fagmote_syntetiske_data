{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade pip > /dev/null\n",
    "!pip install numpy pandas seaborn > /dev/null\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of sampling\n",
    "\n",
    "\n",
    "## Sampling from known distributions\n",
    "We assume that height, weight and income follows a normal distribution. \n",
    "\n",
    "In the following example we assume that:\n",
    "\n",
    "#### Distribution\n",
    "* The mean hight of the population is **180 cm** and the standard deviation is **15 cm**. (Male and female combined)\n",
    "* The mean weight of the population is **80 kg** and the standard deviation is **12 kg**. (Male and female combined)\n",
    "* The mean income is **650000 NOK** and the standard deviation is **50000**.  \n",
    "\n",
    "#### Correlation\n",
    "* The correlation between height and weight is **0.8**\n",
    "* The correlation between **income** and **height** is **0.3**\n",
    "* The correlation between **income** and **weight** is **-0.2** \n",
    "\n",
    "#### _Feel free to tweek these parameters_\n",
    "\n",
    "_Resources:_  \n",
    "https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html  \n",
    "https://towardsdatascience.com/multivariate-normal-distribution-562b28ec0fe0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define statistical properties of population\n",
    "mean_height = 170\n",
    "sd_height = 15\n",
    "\n",
    "mean_weight = 80\n",
    "sd_weight = 12\n",
    "\n",
    "mean_income = 650000\n",
    "sd_income=50000\n",
    "\n",
    "std_matrix = np.array([\n",
    "    [sd_height, 0, 0],\n",
    "    [0, sd_weight, 0],\n",
    "    [0,0, sd_income]\n",
    "])\n",
    "\n",
    "\n",
    "# Specify desired correlation between variables\n",
    "corr_height_weight = 0.8\n",
    "corr_height_income = 0.3\n",
    "corr_weight_income = -0.2\n",
    "\n",
    "# Creating covariance matrix to calculate covariance between distributions.\n",
    "correlation_matrix = np.array([\n",
    "    [1.0, corr_height_weight, corr_height_income],\n",
    "    [corr_height_weight, 1.0, corr_weight_income],\n",
    "    [corr_height_income, corr_weight_income , 1.0]\n",
    "])\n",
    "covariance_matrix = np.dot(std_matrix, np.dot(correlation_matrix, std_matrix))\n",
    "\n",
    "\n",
    "# Sampling from multivariate_distribtuion\n",
    "num_samples = 10000\n",
    "samples = np.random.default_rng().multivariate_normal((mean_height, mean_weight, mean_income), covariance_matrix, num_samples)\n",
    "\n",
    "# Adding samples to Pandas DataFrame\n",
    "samples_df=pd.DataFrame(samples, columns=[\"Height\", \"Weight\", \"Income\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at standard deviations.\n",
    "\n",
    "print(f\"Percentage of people taller than 160 cm: {samples_df[samples_df['Height'] > 160]['Height'].count()/num_samples*100}\")\n",
    "print(f\"Percentage of people shorter than 200 cm: {samples_df[samples_df['Height'] < 200]['Height'].count()/num_samples*100}\")\n",
    "print(f\"Percentage of people earning more than than 750.000 NOK: {samples_df[samples_df['Income'] > 750000]['Income'].count()/num_samples*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print actual descirptive\n",
    "samples_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,3, figsize=(16,8))\n",
    "sns.histplot(samples_df['Height'], ax=ax[0])\n",
    "sns.histplot(samples_df['Weight'], ax=ax[1])\n",
    "sns.histplot(samples_df['Income'], ax=ax[2])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We look at the relationship between height and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=samples_df, x=\"Height\", y=\"Weight\", ax=ax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tallest person\n",
    "print(samples_df.iloc[samples_df[\"Height\"].idxmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest person\n",
    "print(samples_df.iloc[samples_df[\"Height\"].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding deterministic relationships\n",
    "\n",
    "Instead of \"sampling\" or predicting field with deterministic to other other fields we calculate and add them!\n",
    "For example BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bmi(x):\n",
    "    return x[\"Weight\"] / (x[\"Height\"]/100)**2\n",
    "\n",
    "samples_df[\"BMI\"]=samples_df.apply(calculate_bmi,axis=1)\n",
    "\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize ML models\n",
    "\n",
    "Often understanding the relationships in data can be hard to detect. Here is where machine learning shines. An example is sequentially building datasets with machine learning. \n",
    "\n",
    "Consider:\n",
    "1. Sample from $A$ to get $A'$\n",
    "2. Build a model $F1:E \\sim A$\n",
    "3. Synthesize $E$ as $E' = F1(A')$\n",
    "4. Build a model $F2:C \\sim A + E$\n",
    "5. Synthesize $C$ as $C' = F2(A', E')$\n",
    "6. Build a model $F3:B \\sim A + E + C$\n",
    "7. Synthesize $B$ as $B' = F3(A', E' +C')$\n",
    "8. Build a model $F4:D \\sim A + E +C + B$\n",
    "9. Synthesize $D$ as $D' = F4(A', E', C' B')$\n",
    "\n",
    "#### OBS: The method requires access to original data that models the relationship.\n",
    "\n",
    "In this example we use cardio-vascular dataset to model gender from already sampled data. The dataset holds both **height**, **weight** and **gender**. \n",
    "\n",
    "_Resources:_  \n",
    "https://www.kaggle.com/sulianova/cardiovascular-disease-dataset  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Loading dataset to create gender generator\n",
    "with open(\"data/cardio_train.csv\") as file:\n",
    "    data = pd.read_csv(file, delimiter=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting linear regressor to predict gender from height and weight\n",
    "\n",
    "X = data[[\"height\", \"weight\"]]\n",
    "y = data[\"gender\"]\n",
    "clf = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning gender based on height and weight\n",
    "\n",
    "def generate_gender(x):\n",
    "    categories = [\"female\", \"male\"]\n",
    "    probability = clf.predict_proba([[x[\"Height\"],x[\"Weight\"]]])[0]\n",
    "    return np.random.choice(categories, 1, p=probability)[0]\n",
    "\n",
    "samples_df[\"Gender\"]=samples_df.apply(generate_gender,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the count of gender in the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(samples_df['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter deep neural networks\n",
    "\n",
    "When relationships are complicated we can utilize generative techniques from neural netowrks.  Examples of generative techniques are:\n",
    "\n",
    "* **DCGAN** to generate faces: https://keras.io/examples/generative/dcgan_overriding_train_step/\n",
    "* **Autoencoders**: https://www.tensorflow.org/tutorials/generative/autoencoder\n",
    "* **RNN** to generate Shakespear: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ \n",
    "\n",
    "\n",
    "In this example we utilize the technique desciribed in Andrej Karpathys blogpost \"The Unreasonable Effectiveness of Recurrent Neural Networks\". The post is refined here: \n",
    "\n",
    "* https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "## The rational\n",
    "We want to couple a reasonable name to each sample in our population. Let's train a neural network to create plausible names based on gender. \n",
    "\n",
    "To accomplish this we have scraped the 1000 most common names for men and the 1000 most common names for women in Norway. The names are denoted with a special characted in front to indicate if the name is **male** (-) or **female** (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that tensorflow is installed\n",
    "!pip install tensorflow > /dev/null \n",
    "\n",
    "# Import nessesary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Name dataset\"\n",
    "with open(\"data/navn.txt\") as file:\n",
    "    navn = file.read()\n",
    "\n",
    "# Find unique characters in text. Used to encode charactes\n",
    "vocab = sorted(set(navn))\n",
    "\n",
    "# Create functions to convert chars to ints and from ints to chars\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
    "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(navn, input_encoding='UTF-8'))\n",
    "all_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = chars_from_ids(all_ids)\n",
    "all_chars.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from loaded text\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "seq_length = 10\n",
    "examples_per_epoch = len(navn)//(seq_length+1)\n",
    "\n",
    "# Create \"sequences of text\" from full text\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256\n",
    "\n",
    "\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "EPOS = 30 # Change to specify how many times training should occur over. \n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "history = model.fit(dataset, epochs=EPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, next_char, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating initial state from genders in our sample\n",
    "next_char=list(map(lambda x: \"-\" if x == \"male\" else \"*\",samples_df[\"Gender\"].tolist()))\n",
    "\n",
    "# Creating the prediction model. \n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, next_char, temperature=0.5)\n",
    "\n",
    "# Running prediction on batch with size samples_size with length 15 char \n",
    "\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(next_char)\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(15):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(result.numpy())\n",
    "names = list(map(lambda x: x.splitlines()[0].decode(\"utf-8\").replace(\"*\", \"\").replace(\"-\", \"\"), names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df[\"Names\"] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df[samples_df[\"Gender\"]==\"male\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df[samples_df[\"Gender\"]==\"female\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = samples_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
